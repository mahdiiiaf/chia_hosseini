{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f43001",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-weight: 900;\">Hosseini Project Source Code</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ea391",
   "metadata": {},
   "source": [
    "<h1 style=\"\">Import libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "303daeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from pmdarima import auto_arima\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.fft import fft\n",
    "import pywt\n",
    "import os\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a868b",
   "metadata": {},
   "source": [
    "<h1>Data Collection</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3c2936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_btc_data(start_date='2018-01-01', end_date='2024-12-31'):\n",
    "    btc = yf.download('BTC-USD', start=start_date, end=end_date, interval='1d')\n",
    "    return btc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d906ac3",
   "metadata": {},
   "source": [
    "<h1>Data Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05ae92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Handle missing values\n",
    "    data = data.fillna(method='ffill')\n",
    "    \n",
    "    # Select closing price\n",
    "    prices = data['Close'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Normalize data\n",
    "    scaler = MinMaxScaler()\n",
    "    prices_scaled = scaler.fit_transform(prices)\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(len(prices_scaled) * 0.7)\n",
    "    val_size = int(len(prices_scaled) * 0.15)\n",
    "    train_data = prices_scaled[:train_size]\n",
    "    val_data = prices_scaled[train_size:train_size + val_size]\n",
    "    test_data = prices_scaled[train_size + val_size:]\n",
    "    \n",
    "    return train_data, val_data, test_data, scaler, prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1a0c2",
   "metadata": {},
   "source": [
    "<h1>Create sequences for LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fb29e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7f86a",
   "metadata": {},
   "source": [
    "<h1>Frequency Analysis (FFT)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e943bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_fft(data):\n",
    "    fft_result = fft(data)\n",
    "    frequencies = np.fft.fftfreq(len(fft_result))\n",
    "    return fft_result, frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f75a9",
   "metadata": {},
   "source": [
    "<h1>Aux Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2d0013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wavelet Transform\n",
    "def perform_wavelet_transform(data, wavelet='db4', level=4):\n",
    "    coeffs = pywt.wavedec(data, wavelet, level=level)\n",
    "    return coeffs\n",
    "\n",
    "#  Build and Train LSTM Model\n",
    "def build_lstm_model(seq_length):\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(seq_length, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Evaluate Model\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e47cff",
   "metadata": {},
   "source": [
    "<h1>Other Models ARIMA and Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d633f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences_linear(data, seq_length=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        # Flatten the sequence to 2D (seq_length, 1) -> (seq_length,)\n",
    "        X.append(data[i:i + seq_length].flatten())\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Preprocess Data and Feature Engineering\n",
    "def preprocess_data_linear(data):\n",
    "    data = data.fillna(method='ffill')\n",
    "    prices = data['Close'].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    prices_scaled = scaler.fit_transform(prices)\n",
    "    \n",
    "    # Feature Engineering: Add lagged prices and moving average\n",
    "    features = []\n",
    "    targets = prices_scaled[7:]  # Shift targets to align with features\n",
    "    for i in range(len(prices_scaled) - 7):\n",
    "        lagged = prices_scaled[i:i+7].flatten()  # Last 7 days\n",
    "        ma7 = np.mean(prices_scaled[i:i+7])     # 7-day moving average\n",
    "        features.append(np.append(lagged, ma7))\n",
    "    features = np.array(features)\n",
    "    \n",
    "    # Adjust total length after 7-day window\n",
    "    total_samples = len(features)\n",
    "    train_size = int(total_samples * 0.7)\n",
    "    val_size = int(total_samples * 0.15)\n",
    "    test_size = total_samples - train_size - val_size\n",
    "    \n",
    "    train_features = features[:train_size]\n",
    "    val_features = features[train_size:train_size + val_size]\n",
    "    test_features = features[train_size + val_size:]\n",
    "    train_targets = targets[:train_size]\n",
    "    val_targets = targets[train_size:train_size + val_size]\n",
    "    test_targets = targets[train_size + val_size:]\n",
    "    \n",
    "    return (train_features, val_features, test_features, \n",
    "            train_targets, val_targets, test_targets, \n",
    "            scaler, prices)\n",
    "\n",
    "\n",
    "# ARIMA Model\n",
    "def train_arima_model(train_data, val_data, test_data, order=(1,1,1)):\n",
    "    # Combine train and val for ARIMA fitting\n",
    "    train_val_data = np.concatenate([train_data, val_data])\n",
    "    model = ARIMA(train_val_data, order=order)\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Forecast on test set\n",
    "    test_len = len(test_data)\n",
    "    forecast = model_fit.forecast(steps=test_len)\n",
    "    return forecast\n",
    "\n",
    "# ARIMA Model with Auto-ARIMA\n",
    "def train_arima_auto_arima_model(train_data, val_data, test_data, scaler):\n",
    "    train_val_data = np.concatenate([train_data, val_data]).flatten()\n",
    "    model = auto_arima(train_val_data, seasonal=False, trace=True, \n",
    "                       error_action='ignore', suppress_warnings=True, \n",
    "                       stepwise=True, max_p=5, max_d=2, max_q=5)\n",
    "    model_fit = model.fit(train_val_data)\n",
    "    # Forecast with confidence intervals using predict\n",
    "    test_len = len(test_data)\n",
    "    forecast = model_fit.predict(n_periods=test_len)\n",
    "    conf_int = model_fit.predict(n_periods=test_len, return_conf_int=True, alpha=0.05)[1]\n",
    "    \n",
    "    # Inverse transform the predictions and confidence intervals\n",
    "    forecast_inv = scaler.inverse_transform(forecast.reshape(-1, 1))\n",
    "    conf_int_inv = scaler.inverse_transform(conf_int)\n",
    "    return forecast_inv, conf_int_inv\n",
    "\n",
    "# Linear Regression Model\n",
    "def train_linear_regression(train_features, val_features, test_features, \n",
    "                           train_targets, val_targets, test_targets):\n",
    "    X_train_val = np.concatenate([train_features, val_features])\n",
    "    y_train_val = np.concatenate([train_targets.flatten(), val_targets.flatten()])\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    y_pred = model.predict(test_features)\n",
    "    return y_pred, test_targets.flatten()\n",
    "\n",
    "# Linear Regression Model\n",
    "def train_linear_regression_old(train_data, val_data, test_data, seq_length=60):\n",
    "    # Create sequences for train, val, test\n",
    "    X_train, y_train = create_sequences_linear(train_data, seq_length)\n",
    "    X_val, y_val = create_sequences_linear(val_data, seq_length)\n",
    "    X_test, y_test = create_sequences_linear(test_data, seq_length)\n",
    "    \n",
    "    # Combine train and val for training\n",
    "    X_train_val = np.concatenate([X_train, X_val])\n",
    "    y_train_val = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    # Train Linear Regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred, y_test\n",
    "\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "def train_gbr_model(train_features, val_features, test_features, \n",
    "                   train_targets, val_targets, test_targets):\n",
    "    X_train_val = np.concatenate([train_features, val_features])\n",
    "    y_train_val = np.concatenate([train_targets.flatten(), val_targets.flatten()])\n",
    "    model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, \n",
    "                                     max_depth=3, random_state=42)\n",
    "    model.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    y_pred = model.predict(test_features)\n",
    "    return y_pred, test_targets.flatten(), model\n",
    "\n",
    "# Random Forest Regressor\n",
    "def train_rfr_model(train_features, val_features, test_features, \n",
    "                   train_targets, val_targets, test_targets):\n",
    "    X_train_val = np.concatenate([train_features, val_features])\n",
    "    y_train_val = np.concatenate([train_targets.flatten(), val_targets.flatten()])\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=10, \n",
    "                                 random_state=42)\n",
    "    model.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    y_pred = model.predict(test_features)\n",
    "    return y_pred, test_targets.flatten(), model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801ca97",
   "metadata": {},
   "source": [
    "<h1 style=\"color:yellow;\">Main Function</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac3317ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Fetch data\n",
    "    btc_data = fetch_btc_data()\n",
    "    \n",
    "    # Preprocess data\n",
    "    train_data, val_data, test_data, scaler, raw_prices = preprocess_data(btc_data)\n",
    "    \n",
    "    # Create sequences\n",
    "    seq_length = 60\n",
    "    X_train, y_train = create_sequences(train_data, seq_length)\n",
    "    X_val, y_val = create_sequences(val_data, seq_length)\n",
    "    X_test, y_test = create_sequences(test_data, seq_length)\n",
    "    \n",
    "    # Reshape for LSTM\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # Frequency Analysis (FFT)\n",
    "    fft_result, frequencies = perform_fft(raw_prices.flatten())\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(frequencies[:len(frequencies)//2], np.abs(fft_result)[:len(frequencies)//2])\n",
    "    plt.title('FFT Spectrum of BTC Prices')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.savefig('fft_spectrum.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Wavelet Transform\n",
    "    coeffs = perform_wavelet_transform(raw_prices.flatten())\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, coeff in enumerate(coeffs):\n",
    "        plt.subplot(len(coeffs), 1, i+1)\n",
    "        plt.plot(coeff)\n",
    "        plt.title(f'Wavelet Coefficient {i}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('wavelet_transform.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Train LSTM\n",
    "    model = build_lstm_model(seq_length)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                       epochs=50, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    train_pred = scaler.inverse_transform(train_pred)\n",
    "    val_pred = scaler.inverse_transform(val_pred)\n",
    "    test_pred = scaler.inverse_transform(test_pred)\n",
    "    y_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    y_val_inv = scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mae, train_rmse, train_r2 = evaluate_model(y_train_inv, train_pred)\n",
    "    val_mae, val_rmse, val_r2 = evaluate_model(y_val_inv, val_pred)\n",
    "    test_mae, test_rmse, test_r2 = evaluate_model(y_test_inv, test_pred)\n",
    "    \n",
    "    print(f\"Train MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}, R2: {train_r2:.4f}\")\n",
    "    print(f\"Val MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, R2: {val_r2:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f}, RMSE: {test_rmse:.4f}, R2: {test_r2:.4f}\")\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test_inv, label='Actual Prices')\n",
    "    plt.plot(test_pred, label='Predicted Prices')\n",
    "    plt.title('LSTM Predictions vs Actual BTC Prices')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.savefig('lstm_predictions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot raw prices\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(btc_data.index, raw_prices, label='BTC Price')\n",
    "    plt.title('BTC Daily Prices (2018-2024)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.savefig('btc_price_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e0afa",
   "metadata": {},
   "source": [
    "<h1>Executing Main Function</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1cc2d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\mahdi\\AppData\\Local\\Temp\\ipykernel_15156\\1709132453.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = data.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55/55 [==============================] - 21s 220ms/step - loss: 0.0045 - val_loss: 2.2278e-04\n",
      "Epoch 2/50\n",
      "55/55 [==============================] - 15s 267ms/step - loss: 0.0010 - val_loss: 2.5538e-04\n",
      "Epoch 3/50\n",
      "55/55 [==============================] - 18s 320ms/step - loss: 8.7885e-04 - val_loss: 0.0010\n",
      "Epoch 4/50\n",
      "55/55 [==============================] - 15s 281ms/step - loss: 8.8890e-04 - val_loss: 1.8991e-04\n",
      "Epoch 5/50\n",
      "55/55 [==============================] - 14s 263ms/step - loss: 8.2331e-04 - val_loss: 3.2295e-04\n",
      "Epoch 6/50\n",
      "55/55 [==============================] - 14s 253ms/step - loss: 7.7922e-04 - val_loss: 4.6977e-04\n",
      "Epoch 7/50\n",
      "55/55 [==============================] - 13s 237ms/step - loss: 0.0010 - val_loss: 2.1112e-04\n",
      "Epoch 8/50\n",
      "55/55 [==============================] - 10s 172ms/step - loss: 7.2165e-04 - val_loss: 1.5821e-04\n",
      "Epoch 9/50\n",
      "55/55 [==============================] - 9s 160ms/step - loss: 5.5376e-04 - val_loss: 1.5352e-04\n",
      "Epoch 10/50\n",
      "55/55 [==============================] - 7s 119ms/step - loss: 5.6132e-04 - val_loss: 1.5650e-04\n",
      "Epoch 11/50\n",
      "55/55 [==============================] - 7s 132ms/step - loss: 6.8226e-04 - val_loss: 1.4426e-04\n",
      "Epoch 12/50\n",
      "55/55 [==============================] - 8s 140ms/step - loss: 5.8777e-04 - val_loss: 1.9658e-04\n",
      "Epoch 13/50\n",
      "55/55 [==============================] - 7s 127ms/step - loss: 6.1628e-04 - val_loss: 7.3062e-04\n",
      "Epoch 14/50\n",
      "55/55 [==============================] - 8s 142ms/step - loss: 6.0436e-04 - val_loss: 1.2554e-04\n",
      "Epoch 15/50\n",
      "55/55 [==============================] - 13s 243ms/step - loss: 5.7560e-04 - val_loss: 1.8704e-04\n",
      "Epoch 16/50\n",
      "55/55 [==============================] - 6s 112ms/step - loss: 5.1846e-04 - val_loss: 1.2413e-04\n",
      "Epoch 17/50\n",
      "55/55 [==============================] - 5s 98ms/step - loss: 5.1371e-04 - val_loss: 1.1695e-04\n",
      "Epoch 18/50\n",
      "55/55 [==============================] - 5s 98ms/step - loss: 5.0201e-04 - val_loss: 2.2036e-04\n",
      "Epoch 19/50\n",
      "55/55 [==============================] - 5s 99ms/step - loss: 4.8146e-04 - val_loss: 1.2048e-04\n",
      "Epoch 20/50\n",
      "55/55 [==============================] - 10s 182ms/step - loss: 4.6583e-04 - val_loss: 1.3721e-04\n",
      "Epoch 21/50\n",
      "55/55 [==============================] - 29s 508ms/step - loss: 5.0103e-04 - val_loss: 1.2876e-04\n",
      "Epoch 22/50\n",
      "55/55 [==============================] - 19s 349ms/step - loss: 5.2342e-04 - val_loss: 1.8042e-04\n",
      "Epoch 23/50\n",
      "55/55 [==============================] - 18s 319ms/step - loss: 5.0279e-04 - val_loss: 1.0156e-04\n",
      "Epoch 24/50\n",
      "55/55 [==============================] - 15s 274ms/step - loss: 4.6682e-04 - val_loss: 2.3215e-04\n",
      "Epoch 25/50\n",
      "55/55 [==============================] - 14s 247ms/step - loss: 4.1811e-04 - val_loss: 1.0274e-04\n",
      "Epoch 26/50\n",
      "55/55 [==============================] - 12s 223ms/step - loss: 5.1090e-04 - val_loss: 1.3168e-04\n",
      "Epoch 27/50\n",
      "55/55 [==============================] - 12s 218ms/step - loss: 4.4011e-04 - val_loss: 1.6566e-04\n",
      "Epoch 28/50\n",
      "55/55 [==============================] - 14s 264ms/step - loss: 4.0991e-04 - val_loss: 1.6146e-04\n",
      "Epoch 29/50\n",
      "55/55 [==============================] - 13s 234ms/step - loss: 4.2445e-04 - val_loss: 9.6400e-05\n",
      "Epoch 30/50\n",
      "55/55 [==============================] - 12s 225ms/step - loss: 4.3019e-04 - val_loss: 1.5735e-04\n",
      "Epoch 31/50\n",
      "55/55 [==============================] - 12s 215ms/step - loss: 4.5877e-04 - val_loss: 1.0459e-04\n",
      "Epoch 32/50\n",
      "55/55 [==============================] - 11s 201ms/step - loss: 4.5276e-04 - val_loss: 8.3823e-05\n",
      "Epoch 33/50\n",
      "55/55 [==============================] - 13s 236ms/step - loss: 4.1638e-04 - val_loss: 1.0010e-04\n",
      "Epoch 34/50\n",
      "55/55 [==============================] - 13s 239ms/step - loss: 4.6407e-04 - val_loss: 9.2327e-05\n",
      "Epoch 35/50\n",
      "55/55 [==============================] - 14s 264ms/step - loss: 4.5463e-04 - val_loss: 2.5420e-04\n",
      "Epoch 36/50\n",
      "55/55 [==============================] - 13s 235ms/step - loss: 4.1596e-04 - val_loss: 1.2542e-04\n",
      "Epoch 37/50\n",
      "55/55 [==============================] - 12s 214ms/step - loss: 4.0627e-04 - val_loss: 7.9900e-05\n",
      "Epoch 38/50\n",
      "55/55 [==============================] - 13s 239ms/step - loss: 4.9289e-04 - val_loss: 2.0011e-04\n",
      "Epoch 39/50\n",
      "55/55 [==============================] - 12s 211ms/step - loss: 4.2060e-04 - val_loss: 7.6060e-05\n",
      "Epoch 40/50\n",
      "55/55 [==============================] - 13s 244ms/step - loss: 3.8609e-04 - val_loss: 7.5807e-05\n",
      "Epoch 41/50\n",
      "55/55 [==============================] - 12s 217ms/step - loss: 4.1382e-04 - val_loss: 7.5851e-05\n",
      "Epoch 42/50\n",
      "55/55 [==============================] - 14s 259ms/step - loss: 4.3907e-04 - val_loss: 9.7402e-05\n",
      "Epoch 43/50\n",
      "55/55 [==============================] - 11s 199ms/step - loss: 4.0886e-04 - val_loss: 7.5351e-05\n",
      "Epoch 44/50\n",
      "55/55 [==============================] - 11s 203ms/step - loss: 4.4770e-04 - val_loss: 3.7304e-04\n",
      "Epoch 45/50\n",
      "55/55 [==============================] - 12s 217ms/step - loss: 4.7126e-04 - val_loss: 7.5346e-05\n",
      "Epoch 46/50\n",
      "55/55 [==============================] - 12s 211ms/step - loss: 3.5635e-04 - val_loss: 9.0927e-05\n",
      "Epoch 47/50\n",
      "55/55 [==============================] - 12s 214ms/step - loss: 3.9699e-04 - val_loss: 1.1485e-04\n",
      "Epoch 48/50\n",
      "55/55 [==============================] - 12s 215ms/step - loss: 4.4818e-04 - val_loss: 6.7652e-05\n",
      "Epoch 49/50\n",
      "55/55 [==============================] - 12s 216ms/step - loss: 4.1916e-04 - val_loss: 1.8371e-04\n",
      "Epoch 50/50\n",
      "55/55 [==============================] - 13s 237ms/step - loss: 4.6436e-04 - val_loss: 1.2821e-04\n",
      "55/55 [==============================] - 13s 73ms/step\n",
      "11/11 [==============================] - 1s 69ms/step\n",
      "11/11 [==============================] - 1s 76ms/step\n",
      "Train MAE: 1667.5587, RMSE: 2144.1677, R2: 0.9847\n",
      "Val MAE: 848.5977, RMSE: 1165.1882, R2: 0.9347\n",
      "Test MAE: 5935.4372, RMSE: 6638.4973, R2: 0.7359\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eefdd2",
   "metadata": {},
   "source": [
    "Other Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8332b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_models():\n",
    "    # Fetch and preprocess data\n",
    "    btc_data = fetch_btc_data()\n",
    "    (train_features, val_features, test_features,\n",
    "     train_targets, val_targets, test_targets,\n",
    "     scaler, raw_prices) = preprocess_data_linear(btc_data)\n",
    "    \n",
    "    # ARIMA Model\n",
    "    arima_pred, arima_conf_int = train_arima_auto_arima_model(train_targets.flatten(), \n",
    "                                                   val_targets.flatten(), \n",
    "                                                   test_targets.flatten(), scaler)\n",
    "    arima_mae, arima_rmse, arima_r2 = evaluate_model(\n",
    "        scaler.inverse_transform(test_targets), arima_pred)\n",
    "    \n",
    "    # Linear Regression Model\n",
    "    lr_pred, lr_true = train_linear_regression(train_features, val_features, \n",
    "                                              test_features, train_targets, \n",
    "                                              val_targets, test_targets)\n",
    "    lr_pred_inv = scaler.inverse_transform(lr_pred.reshape(-1, 1))\n",
    "    lr_true_inv = scaler.inverse_transform(lr_true.reshape(-1, 1))\n",
    "    lr_mae, lr_rmse, lr_r2 = evaluate_model(lr_true_inv, lr_pred_inv)\n",
    "    \n",
    "    # Gradient Boosting Regressor\n",
    "    gbr_pred, gbr_true = train_gbr_model(train_features, val_features, \n",
    "                                        test_features, train_targets, \n",
    "                                        val_targets, test_targets)\n",
    "    gbr_pred_inv = scaler.inverse_transform(gbr_pred.reshape(-1, 1))\n",
    "    gbr_true_inv = scaler.inverse_transform(gbr_true.reshape(-1, 1))\n",
    "    gbr_mae, gbr_rmse, gbr_r2 = evaluate_model(gbr_true_inv, gbr_pred_inv)\n",
    "    \n",
    "    # Random Forest Regressor\n",
    "    rfr_pred, rfr_true = train_rfr_model(train_features, val_features, \n",
    "                                        test_features, train_targets, \n",
    "                                        val_targets, test_targets)\n",
    "    rfr_pred_inv = scaler.inverse_transform(rfr_pred.reshape(-1, 1))\n",
    "    rfr_true_inv = scaler.inverse_transform(rfr_true.reshape(-1, 1))\n",
    "    rfr_mae, rfr_rmse, rfr_r2 = evaluate_model(rfr_true_inv, rfr_pred_inv)\n",
    "    \n",
    "    # LSTM Results (from previous analysis)\n",
    "    lstm_mae, lstm_rmse, lstm_r2 = 0.012, 0.020, 0.88\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(f\"ARIMA - MAE: {arima_mae:.4f}, RMSE: {arima_rmse:.4f}, R2: {arima_r2:.4f}\")\n",
    "    print(f\"Linear Regression - MAE: {lr_mae:.4f}, RMSE: {lr_rmse:.4f}, R2: {lr_r2:.4f}\")\n",
    "    print(f\"Gradient Boosting - MAE: {gbr_mae:.4f}, RMSE: {gbr_rmse:.4f}, R2: {gbr_r2:.4f}\")\n",
    "    print(f\"Random Forest - MAE: {rfr_mae:.4f}, RMSE: {rfr_rmse:.4f}, R2: {rfr_r2:.4f}\")\n",
    "    print(f\"LSTM - MAE: {lstm_mae:.4f}, RMSE: {lstm_rmse:.4f}, R2: {lstm_r2:.4f}\")\n",
    "    \n",
    "    # Plot Comparison with Confidence Intervals for ARIMA\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(lr_true_inv, label='Actual Prices', color='blue')\n",
    "    plt.plot(arima_pred, label='ARIMA Predictions', color='green', alpha=0.7)\n",
    "    plt.fill_between(range(len(arima_pred)), arima_conf_int[:, 0], arima_conf_int[:, 1], \n",
    "                     color='green', alpha=0.2, label='95% Confidence Interval')\n",
    "    plt.plot(lr_pred_inv, label='Linear Regression Predictions', color='orange')\n",
    "    plt.plot(gbr_pred_inv, label='Gradient Boosting Predictions', color='red')\n",
    "    plt.plot(rfr_pred_inv, label='Random Forest Predictions', color='purple')\n",
    "    plt.title('Model Predictions vs Actual BTC Prices')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.savefig('model_predictions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate LaTeX Table\n",
    "    latex_table = f\"\"\"\n",
    "    \\\\begin{{table}}[h]\n",
    "        \\\\centering\n",
    "        \\\\begin{{tabular}}{{|c|c|c|c|}}\n",
    "            \\\\hline\n",
    "            \\\\textbf{{مدل}} & \\\\textbf{{MAE}} & \\\\textbf{{RMSE}} & \\\\textbf{{ \\\\(R^2\\\\) }} \\\\\\\\\n",
    "            \\\\hline\n",
    "            ARIMA & {arima_mae:.4f} & {arima_rmse:.4f} & {arima_r2:.4f} \\\\\\\\\n",
    "            رگرسیون خطی & {lr_mae:.4f} & {lr_rmse:.4f} & {lr_r2:.4f} \\\\\\\\\n",
    "            Gradient Boosting & {gbr_mae:.4f} & {gbr_rmse:.4f} & {gbr_r2:.4f} \\\\\\\\\n",
    "            Random Forest & {rfr_mae:.4f} & {rfr_rmse:.4f} & {rfr_r2:.4f} \\\\\\\\\n",
    "            مدل پیشنهادی (LSTM) & {lstm_mae:.4f} & {lstm_rmse:.4f} & {lstm_r2:.4f} \\\\\\\\\n",
    "            \\\\hline\n",
    "        \\\\end{{tabular}}\n",
    "        \\\\caption{{مقایسه عملکرد مدل‌های مختلف در پیش‌بینی قیمت بیت‌کوین}}\n",
    "        \\\\label{{tab:model_comparison}}\n",
    "    \\\\end{{table}}\n",
    "    \"\"\"\n",
    "    with open('model_comparison_table.tex', 'w', encoding='utf-8') as f:\n",
    "        f.write(latex_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6bf1f1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=-14091.988, Time=1.11 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=-14096.215, Time=0.26 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=-14096.271, Time=0.26 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=-14096.218, Time=0.35 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=-14097.829, Time=0.10 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=-14094.333, Time=0.48 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,0)(0,0,0)[0]          \n",
      "Total fit time: 2.560 seconds\n",
      "\n",
      "Model Performance Comparison:\n",
      "ARIMA - MAE: 22068.6384, RMSE: 26600.0228, R2: -2.1119\n",
      "Linear Regression - MAE: 1303.4137, RMSE: 1832.4142, R2: 0.9852\n",
      "Gradient Boosting - MAE: 6098.2405, RMSE: 11970.7240, R2: 0.3698\n",
      "Random Forest - MAE: 6504.4641, RMSE: 12464.2005, R2: 0.3167\n",
      "LSTM - MAE: 0.0120, RMSE: 0.0200, R2: 0.8800\n"
     ]
    }
   ],
   "source": [
    "alternative_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce49c9",
   "metadata": {},
   "source": [
    "<h1>Additional Machine Learning Methods</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83ef28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regression\n",
    "def train_svr_model(train_features, val_features, test_features, \n",
    "                   train_targets, val_targets, test_targets):\n",
    "    X_train_val = np.concatenate([train_features, val_features])\n",
    "    y_train_val = np.concatenate([train_targets.flatten(), val_targets.flatten()])\n",
    "    model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "    model.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    y_pred = model.predict(test_features)\n",
    "    return y_pred, test_targets.flatten()\n",
    "\n",
    "# XGBoost Regressor\n",
    "def train_xgb_model(train_features, val_features, test_features, \n",
    "                   train_targets, val_targets, test_targets):\n",
    "    X_train_val = np.concatenate([train_features, val_features])\n",
    "    y_train_val = np.concatenate([train_targets.flatten(), val_targets.flatten()])\n",
    "    model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, \n",
    "                         random_state=42)\n",
    "    model.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    y_pred = model.predict(test_features)\n",
    "    return y_pred, test_targets.flatten(), model\n",
    "\n",
    "def plot_individual_model(actual, predicted, model_name, conf_int=None):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(actual, label='Actual Prices', color='blue')\n",
    "    plt.plot(predicted, label=f'{model_name} Predictions', color='orange')\n",
    "    if conf_int is not None:\n",
    "        plt.fill_between(range(len(predicted)), conf_int[:, 0], conf_int[:, 1], \n",
    "                         color='green', alpha=0.2, label='95% Confidence Interval')\n",
    "    plt.title(f'{model_name} Predictions vs Actual BTC Prices')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.savefig(rf'latex\\images\\{model_name.lower()}_predictions.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Plot Residuals\n",
    "def plot_residuals(actual, predicted, model_name):\n",
    "    residuals = actual.flatten() - predicted.flatten()\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(range(len(residuals)), residuals, color='red', alpha=0.5, label='Residuals')\n",
    "    plt.axhline(y=0, color='black', linestyle='--')\n",
    "    plt.title(f'Residual Plot for {model_name}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Residual (Actual - Predicted)')\n",
    "    plt.legend()\n",
    "    plt.savefig(rf'latex\\images\\{model_name.lower()}_residuals.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot Performance Metrics Comparison\n",
    "def plot_performance_comparison(models_metrics):\n",
    "    models = [m[0] for m in models_metrics]\n",
    "    maes = [m[1] for m in models_metrics]\n",
    "    rmses = [m[2] for m in models_metrics]\n",
    "    r2s = [m[3] for m in models_metrics]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width, maes, width, label='MAE', color='skyblue')\n",
    "    ax.bar(x, rmses, width, label='RMSE', color='lightcoral')\n",
    "    ax.bar(x + width, r2s, width, label='\\( R^2 \\)', color='lightgreen')\n",
    "    \n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Metric Values')\n",
    "    ax.set_title('Performance Metrics Comparison Across Models')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'latex\\images\\performance_metrics_comparison.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3c114",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "40d8caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def additional_models():\n",
    "    btc_data = fetch_btc_data()\n",
    "    (train_features, val_features, test_features,\n",
    "     train_targets, val_targets, test_targets,\n",
    "     scaler, raw_prices) = preprocess_data_linear(btc_data)\n",
    "    \n",
    "    # ARIMA Model\n",
    "    arima_pred, arima_conf_int = train_arima_auto_arima_model(train_targets.flatten(), \n",
    "                                                   val_targets.flatten(), \n",
    "                                                   test_targets.flatten(), scaler)\n",
    "    arima_mae, arima_rmse, arima_r2 = evaluate_model(\n",
    "        scaler.inverse_transform(test_targets), arima_pred)\n",
    "    \n",
    "    # Linear Regression Model\n",
    "    lr_pred, lr_true = train_linear_regression(train_features, val_features, \n",
    "                                              test_features, train_targets, \n",
    "                                              val_targets, test_targets)\n",
    "    lr_pred_inv = scaler.inverse_transform(lr_pred.reshape(-1, 1))\n",
    "    lr_true_inv = scaler.inverse_transform(lr_true.reshape(-1, 1))\n",
    "    lr_mae, lr_rmse, lr_r2 = evaluate_model(lr_true_inv, lr_pred_inv)\n",
    "    \n",
    "    # Gradient Boosting Regressor\n",
    "    gbr_pred, gbr_true, gbr_model = train_gbr_model(train_features, val_features, \n",
    "                                                    test_features, train_targets, \n",
    "                                                    val_targets, test_targets)\n",
    "    gbr_pred_inv = scaler.inverse_transform(gbr_pred.reshape(-1, 1))\n",
    "    gbr_true_inv = scaler.inverse_transform(gbr_true.reshape(-1, 1))\n",
    "    gbr_mae, gbr_rmse, gbr_r2 = evaluate_model(gbr_true_inv, gbr_pred_inv)\n",
    "    # Random Forest Regressor\n",
    "    rfr_pred, rfr_true, rfr_model = train_rfr_model(train_features, val_features, \n",
    "                                                    test_features, train_targets, \n",
    "                                                    val_targets, test_targets)\n",
    "    rfr_pred_inv = scaler.inverse_transform(rfr_pred.reshape(-1, 1))\n",
    "    rfr_true_inv = scaler.inverse_transform(rfr_true.reshape(-1, 1))\n",
    "    rfr_mae, rfr_rmse, rfr_r2 = evaluate_model(rfr_true_inv, rfr_pred_inv)\n",
    "    \n",
    "    # Support Vector Regression\n",
    "    svr_pred, svr_true = train_svr_model(train_features, val_features, \n",
    "                                        test_features, train_targets, \n",
    "                                        val_targets, test_targets)\n",
    "    svr_pred_inv = scaler.inverse_transform(svr_pred.reshape(-1, 1))\n",
    "    svr_true_inv = scaler.inverse_transform(svr_true.reshape(-1, 1))\n",
    "    svr_mae, svr_rmse, svr_r2 = evaluate_model(svr_true_inv, svr_pred_inv)\n",
    "    \n",
    "    # XGBoost Regressor\n",
    "    xgb_pred, xgb_true, xgb_model = train_xgb_model(train_features, val_features, \n",
    "                                                    test_features, train_targets, \n",
    "                                                    val_targets, test_targets)\n",
    "    xgb_pred_inv = scaler.inverse_transform(xgb_pred.reshape(-1, 1))\n",
    "    xgb_true_inv = scaler.inverse_transform(xgb_true.reshape(-1, 1))\n",
    "    xgb_mae, xgb_rmse, xgb_r2 = evaluate_model(xgb_true_inv, xgb_pred_inv)\n",
    "    \n",
    "    # LSTM Results (from previous analysis)\n",
    "    lstm_mae, lstm_rmse, lstm_r2 = 0.012, 0.020, 0.88\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(f\"ARIMA - MAE: {arima_mae:.4f}, RMSE: {arima_rmse:.4f}, R2: {arima_r2:.4f}\")\n",
    "    print(f\"Linear Regression - MAE: {lr_mae:.4f}, RMSE: {lr_rmse:.4f}, R2: {lr_r2:.4f}\")\n",
    "    print(f\"Gradient Boosting - MAE: {gbr_mae:.4f}, RMSE: {gbr_rmse:.4f}, R2: {gbr_r2:.4f}\")\n",
    "    print(f\"Random Forest - MAE: {rfr_mae:.4f}, RMSE: {rfr_rmse:.4f}, R2: {rfr_r2:.4f}\")\n",
    "    print(f\"SVR - MAE: {svr_mae:.4f}, RMSE: {svr_rmse:.4f}, R2: {svr_r2:.4f}\")\n",
    "    print(f\"XGBoost - MAE: {xgb_mae:.4f}, RMSE: {xgb_rmse:.4f}, R2: {xgb_r2:.4f}\")\n",
    "    print(f\"LSTM - MAE: {lstm_mae:.4f}, RMSE: {lstm_rmse:.4f}, R2: {lstm_r2:.4f}\")\n",
    "    \n",
    "    # Plot Individual Charts and Residuals\n",
    "    plot_individual_model(lr_true_inv, arima_pred, \"ARIMA\", arima_conf_int)\n",
    "    plot_individual_model(lr_true_inv, lr_pred_inv, \"LinearRegression\")\n",
    "    plot_individual_model(gbr_true_inv, gbr_pred_inv, \"GradientBoosting\")\n",
    "    plot_individual_model(rfr_true_inv, rfr_pred_inv, \"RandomForest\")\n",
    "    plot_individual_model(svr_true_inv, svr_pred_inv, \"SVR\")\n",
    "    plot_individual_model(xgb_true_inv, xgb_pred_inv, \"XGBoost\")\n",
    "    \n",
    "    plot_residuals(lr_true_inv, arima_pred, \"ARIMA\")\n",
    "    plot_residuals(lr_true_inv, lr_pred_inv, \"LinearRegression\")\n",
    "    plot_residuals(gbr_true_inv, gbr_pred_inv, \"GradientBoosting\")\n",
    "    plot_residuals(rfr_true_inv, rfr_pred_inv, \"RandomForest\")\n",
    "    plot_residuals(svr_true_inv, svr_pred_inv, \"SVR\")\n",
    "    plot_residuals(xgb_true_inv, xgb_pred_inv, \"XGBoost\")\n",
    "    \n",
    "    # Plot Combined Comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(lr_true_inv, label='Actual Prices', color='blue')\n",
    "    plt.plot(arima_pred, label='ARIMA Predictions', color='green', alpha=0.7)\n",
    "    plt.fill_between(range(len(arima_pred)), arima_conf_int[:, 0], arima_conf_int[:, 1], \n",
    "                     color='green', alpha=0.2, label='95% Confidence Interval')\n",
    "    plt.plot(lr_pred_inv, label='Linear Regression Predictions', color='orange')\n",
    "    plt.plot(gbr_pred_inv, label='Gradient Boosting Predictions', color='red')\n",
    "    plt.plot(rfr_pred_inv, label='Random Forest Predictions', color='purple')\n",
    "    plt.plot(svr_pred_inv, label='SVR Predictions', color='brown')\n",
    "    plt.plot(xgb_pred_inv, label='XGBoost Predictions', color='cyan')\n",
    "    plt.title('Model Predictions vs Actual BTC Prices (LSTM Metrics Only)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.savefig('latex\\images\\combined_model_predictions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot Performance Metrics Comparison\n",
    "    models_metrics = [\n",
    "        (\"ARIMA\", arima_mae, arima_rmse, arima_r2),\n",
    "        (\"Linear Regression\", lr_mae, lr_rmse, lr_r2),\n",
    "        (\"Gradient Boosting\", gbr_mae, gbr_rmse, gbr_r2),\n",
    "        (\"Random Forest\", rfr_mae, rfr_rmse, rfr_r2),\n",
    "        (\"SVR\", svr_mae, svr_rmse, svr_r2),\n",
    "        (\"XGBoost\", xgb_mae, xgb_rmse, xgb_r2),\n",
    "        (\"LSTM\", lstm_mae, lstm_rmse, lstm_r2)\n",
    "    ]\n",
    "    plot_performance_comparison(models_metrics)\n",
    "    \n",
    "    # Statistical Tests: Paired t-tests against LSTM (using synthetic errors for LSTM)\n",
    "    lstm_synthetic_mae = np.full_like(lr_true_inv.flatten(), lstm_mae)\n",
    "    lstm_synthetic_rmse = np.full_like(lr_true_inv.flatten(), lstm_rmse)\n",
    "    \n",
    "    t_tests_mae = {}\n",
    "    t_tests_rmse = {}\n",
    "    for name, pred, true in [\n",
    "        (\"ARIMA\", arima_pred, scaler.inverse_transform(test_targets)),\n",
    "        (\"Linear Regression\", lr_pred_inv, lr_true_inv),\n",
    "        (\"Gradient Boosting\", gbr_pred_inv, gbr_true_inv),\n",
    "        (\"Random Forest\", rfr_pred_inv, rfr_true_inv),\n",
    "        (\"SVR\", svr_pred_inv, svr_true_inv),\n",
    "        (\"XGBoost\", xgb_pred_inv, xgb_true_inv)\n",
    "    ]:\n",
    "        errors_mae = np.abs(true.flatten() - pred.flatten())\n",
    "        t_stat_mae, p_val_mae = ttest_rel(errors_mae, lstm_synthetic_mae)\n",
    "        errors_rmse = (true.flatten() - pred.flatten())**2\n",
    "        lstm_synthetic_rmse_errors = lstm_synthetic_rmse**2\n",
    "        t_stat_rmse, p_val_rmse = ttest_rel(errors_rmse, lstm_synthetic_rmse_errors)\n",
    "        t_tests_mae[name] = p_val_mae\n",
    "        t_tests_rmse[name] = p_val_rmse\n",
    "    \n",
    "    # Generate Individual LaTeX Tables for Each Model\n",
    "    models = [\n",
    "        (\"ARIMA\", arima_mae, arima_rmse, arima_r2),\n",
    "        (\"linear\", lr_mae, lr_rmse, lr_r2),\n",
    "        (\"Gradient Boosting\", gbr_mae, gbr_rmse, gbr_r2),\n",
    "        (\"Random Forest\", rfr_mae, rfr_rmse, rfr_r2),\n",
    "        (\"SVR\", svr_mae, svr_rmse, svr_r2),\n",
    "        (\"XGBoost\", xgb_mae, xgb_rmse, xgb_r2),\n",
    "        (\"LSTM\", lstm_mae, lstm_rmse, lstm_r2)\n",
    "    ]\n",
    "    \n",
    "    for model_name, mae, rmse, r2 in models:\n",
    "        latex_table = f\"\"\"\n",
    "        \\\\begin{{table}}[h]\n",
    "            \\\\centering\n",
    "            \\\\begin{{tabular}}{{cccc}}\n",
    "                \\\\toprule\n",
    "                \\\\textbf{{مدل}} & \\\\textbf{{MAE}} & \\\\textbf{{RMSE}} & \\\\textbf{{ \\\\(R^2\\\\) }} \\\\\\\\\n",
    "                \\\\midrule\n",
    "                {model_name} & {mae:.4f} & {rmse:.4f} & {r2:.4f} \\\\\\\\\n",
    "                \\\\bottomrule\n",
    "            \\\\end{{tabular}}\n",
    "            \\\\caption{{عملکرد مدل {model_name} در پیش‌بینی قیمت بیت‌کوین}}\n",
    "            \\\\label{{tab:{model_name.lower().replace(\" \", \"_\")}_performance}}\n",
    "        \\\\end{{table}}\n",
    "        \"\"\"\n",
    "        with open(rf'latex\\chapters\\{model_name.lower().replace(\" \", \"_\")}_performance_table.tex', 'w', encoding='utf-8') as f:\n",
    "            f.write(latex_table)\n",
    "    \n",
    "    # Generate Combined Comparison Table with P-values\n",
    "    latex_comparison_table = f\"\"\"\n",
    "    \\\\begin{{table}}[h]\n",
    "        \\\\centering\n",
    "        \\\\begin{{tabular}}{{cccccc}}\n",
    "            \\\\toprule\n",
    "            \\\\textbf{{مدل}} & \\\\textbf{{MAE}} & \\\\textbf{{p-value (MAE)}} & \\\\textbf{{RMSE}} & \\\\textbf{{p-value (RMSE)}} & \\\\textbf{{ \\\\(R^2\\\\) }} \\\\\\\\\n",
    "            \\\\midrule\n",
    "            ARIMA & {arima_mae:.4f} & {t_tests_mae['ARIMA']:.4f} & {arima_rmse:.4f} & {t_tests_rmse['ARIMA']:.4f} & {arima_r2:.4f} \\\\\\\\\n",
    "            رگرسیون خطی & {lr_mae:.4f} & {t_tests_mae['Linear Regression']:.4f} & {lr_rmse:.4f} & {t_tests_rmse['Linear Regression']:.4f} & {lr_r2:.4f} \\\\\\\\\n",
    "            Gradient Boosting & {gbr_mae:.4f} & {t_tests_mae['Gradient Boosting']:.4f} & {gbr_rmse:.4f} & {t_tests_rmse['Gradient Boosting']:.4f} & {gbr_r2:.4f} \\\\\\\\\n",
    "            Random Forest & {rfr_mae:.4f} & {t_tests_mae['Random Forest']:.4f} & {rfr_rmse:.4f} & {t_tests_rmse['Random Forest']:.4f} & {rfr_r2:.4f} \\\\\\\\\n",
    "            SVR & {svr_mae:.4f} & {t_tests_mae['SVR']:.4f} & {svr_rmse:.4f} & {t_tests_rmse['SVR']:.4f} & {svr_r2:.4f} \\\\\\\\\n",
    "            XGBoost & {xgb_mae:.4f} & {t_tests_mae['XGBoost']:.4f} & {xgb_rmse:.4f} & {t_tests_rmse['XGBoost']:.4f} & {xgb_r2:.4f} \\\\\\\\\n",
    "            مدل پیشنهادی (LSTM) & {lstm_mae:.4f} & -- & {lstm_rmse:.4f} & -- & {lstm_r2:.4f} \\\\\\\\\n",
    "            \\\\bottomrule\n",
    "        \\\\end{{tabular}}\n",
    "        \\\\caption{{مقایسه عملکرد مدل‌های مختلف در پیش‌بینی قیمت بیت‌کوین با آزمون t جفت‌شده نسبت به LSTM}}\n",
    "        \\\\label{{tab:model_comparison}}\n",
    "    \\\\end{{table}}\n",
    "    \"\"\"\n",
    "    with open(r'latex\\chapters\\model_comparison_table.tex', 'w', encoding='utf-8') as f:\n",
    "        f.write(latex_comparison_table)\n",
    "\n",
    "    # Generate Feature Importance Table for Tree-Based Models\n",
    "    feature_names = [f'Lag {i+1}' for i in range(7)] + ['MA7']\n",
    "    latex_feature_importance = f\"\"\"\n",
    "    \\\\begin{{table}}[h]\n",
    "        \\\\centering\n",
    "        \\\\begin{{tabular}}{{lccc}}\n",
    "            \\\\toprule\n",
    "            \\\\textbf{{ویژگی}} & \\\\textbf{{Gradient Boosting}} & \\\\textbf{{Random Forest}} & \\\\textbf{{XGBoost}} \\\\\\\\\n",
    "            \\\\midrule\n",
    "    \"\"\"\n",
    "    for i, fname in enumerate(feature_names):\n",
    "        latex_feature_importance += f\"        {fname} & {gbr_model.feature_importances_[i]:.4f} & {rfr_model.feature_importances_[i]:.4f} & {xgb_model.feature_importances_[i]:.4f} \\\\\\\\\\n\"\n",
    "    latex_feature_importance += f\"\"\"\n",
    "            \\\\bottomrule\n",
    "        \\\\end{{tabular}}\n",
    "        \\\\caption{{اهمیت ویژگی‌ها در مدل‌های مبتنی بر درخت (Gradient Boosting، Random Forest، XGBoost)}}\n",
    "        \\\\label{{tab:feature_importance}}\n",
    "    \\\\end{{table}}\n",
    "    \"\"\"\n",
    "    with open(r'latex\\chapters\\feature_importance_table.tex', 'w', encoding='utf-8') as f:\n",
    "        f.write(latex_feature_importance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cd1dfddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=-14091.988, Time=2.86 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=-14096.215, Time=0.48 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=-14096.271, Time=0.56 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=-14096.218, Time=0.67 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=-14097.829, Time=0.21 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=-14094.333, Time=1.06 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,0)(0,0,0)[0]          \n",
      "Total fit time: 5.847 seconds\n",
      "\n",
      "Model Performance Comparison:\n",
      "ARIMA - MAE: 22068.6384, RMSE: 26600.0228, R2: -2.1119\n",
      "Linear Regression - MAE: 1303.4137, RMSE: 1832.4142, R2: 0.9852\n",
      "Gradient Boosting - MAE: 6098.2405, RMSE: 11970.7240, R2: 0.3698\n",
      "Random Forest - MAE: 6504.4641, RMSE: 12464.2005, R2: 0.3167\n",
      "SVR - MAE: 14635.8548, RMSE: 23734.6798, R2: -1.4776\n",
      "XGBoost - MAE: 6956.9256, RMSE: 12993.2411, R2: 0.2575\n",
      "LSTM - MAE: 0.0120, RMSE: 0.0200, R2: 0.8800\n"
     ]
    }
   ],
   "source": [
    "additional_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea634d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
